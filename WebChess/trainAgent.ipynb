{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9f54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import chess\n",
    "import chess.pgn\n",
    "from trainAgentOnFly import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a99fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Connect to your database\n",
    "conn = sqlite3.connect('chess_games.db')\n",
    "\n",
    "# 2. Write SQL query to JOIN moves and sessions\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    moves.session_id,\n",
    "    moves.move_number,\n",
    "    moves.move_uci,\n",
    "    moves.player,\n",
    "    moves.fen,\n",
    "    moves.timestamp,\n",
    "    sessions.result\n",
    "FROM moves\n",
    "JOIN sessions ON moves.session_id = sessions.session_id\n",
    "ORDER BY moves.session_id, moves.move_number\n",
    "\"\"\"\n",
    "\n",
    "# 3. Read the result into a Pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# 4. Check the data\n",
    "# print(df.head())\n",
    "\n",
    "# 5. OPTIONAL: Map result to reward\n",
    "# For example, player win -> 1, agent win -> -1, draw -> 0\n",
    "df = add_step_rewards(df)\n",
    "\n",
    "# 6. Save prepared data to CSV if needed\n",
    "# df.to_csv(\"chess_moves_with_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cdae205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b96750",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0d186b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  2.,  3.,  5.,  6.,  3.,  0.,  4.],\n",
       "        [ 0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  2.],\n",
       "        [ 0.,  0.,  0.,  7.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  9.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0., 11.,  0.,  0.],\n",
       "        [ 7.,  7.,  7.,  7.,  0.,  7.,  7.,  7.],\n",
       "        [10.,  8.,  9.,  0., 12.,  0.,  8., 10.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 7\n",
    "encode_fen(data[z][\"state\"]).reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0654d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7b5\n",
      "-0.01\n"
     ]
    }
   ],
   "source": [
    "print(data[z][\"action\"])\n",
    "print(data[z][\"reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae12a55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  2.,  3.,  5.,  6.,  3.,  0.,  4.],\n",
       "        [ 0.,  0.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  2.],\n",
       "        [ 0.,  1.,  0.,  7.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  9.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0., 11.,  0.,  0.],\n",
       "        [ 7.,  7.,  7.,  7.,  0.,  7.,  7.,  7.],\n",
       "        [10.,  8.,  9.,  0., 12.,  0.,  8., 10.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_fen(data[z][\"next_state\"]).reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3471948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_fen(data[5][\"next_state\"]).reshape((8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f67ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sessions in the dataset: 54\n",
      "Epoch 1/10000 | Loss: 0.2104\n",
      "Epoch 11/10000 | Loss: 0.1926\n",
      "Epoch 21/10000 | Loss: 0.0465\n",
      "Epoch 31/10000 | Loss: 0.0105\n",
      "Epoch 41/10000 | Loss: 0.0013\n",
      "Epoch 51/10000 | Loss: 0.0045\n",
      "Epoch 61/10000 | Loss: 0.0287\n",
      "Epoch 71/10000 | Loss: 0.0046\n",
      "Epoch 81/10000 | Loss: 0.0060\n",
      "Epoch 91/10000 | Loss: 0.0009\n",
      "Epoch 101/10000 | Loss: 0.0017\n",
      "Epoch 111/10000 | Loss: 0.0056\n",
      "Epoch 121/10000 | Loss: 0.0048\n",
      "Epoch 131/10000 | Loss: 0.0010\n",
      "Epoch 141/10000 | Loss: 0.0070\n",
      "Epoch 151/10000 | Loss: 0.0175\n",
      "Epoch 161/10000 | Loss: 0.0078\n",
      "Epoch 171/10000 | Loss: 0.0012\n",
      "Epoch 181/10000 | Loss: 0.0019\n",
      "Epoch 191/10000 | Loss: 0.0128\n",
      "Epoch 201/10000 | Loss: 0.0016\n",
      "Epoch 211/10000 | Loss: 0.0018\n",
      "Epoch 221/10000 | Loss: 0.0033\n",
      "Epoch 231/10000 | Loss: 0.0084\n",
      "Epoch 241/10000 | Loss: 0.0013\n",
      "Epoch 251/10000 | Loss: 0.0013\n",
      "Epoch 261/10000 | Loss: 0.0008\n",
      "Epoch 271/10000 | Loss: 0.0290\n",
      "Epoch 281/10000 | Loss: 0.0065\n",
      "Epoch 291/10000 | Loss: 0.0014\n",
      "Epoch 301/10000 | Loss: 0.0053\n",
      "Epoch 311/10000 | Loss: 0.0011\n",
      "Epoch 321/10000 | Loss: 0.0074\n",
      "Epoch 331/10000 | Loss: 0.0015\n",
      "Epoch 341/10000 | Loss: 0.0016\n",
      "Epoch 351/10000 | Loss: 0.0113\n",
      "Epoch 361/10000 | Loss: 0.0009\n",
      "Epoch 371/10000 | Loss: 0.0003\n",
      "Epoch 381/10000 | Loss: 0.0131\n",
      "Epoch 391/10000 | Loss: 0.0011\n",
      "Epoch 401/10000 | Loss: 0.0010\n",
      "Epoch 411/10000 | Loss: 0.0077\n",
      "Epoch 421/10000 | Loss: 0.0014\n",
      "Epoch 431/10000 | Loss: 0.0042\n",
      "Epoch 441/10000 | Loss: 0.0012\n",
      "Epoch 451/10000 | Loss: 0.0004\n",
      "Epoch 461/10000 | Loss: 0.0014\n",
      "Epoch 471/10000 | Loss: 0.0015\n",
      "Epoch 481/10000 | Loss: 0.0015\n",
      "Epoch 491/10000 | Loss: 0.0041\n",
      "Epoch 501/10000 | Loss: 0.0013\n",
      "Epoch 511/10000 | Loss: 0.0003\n",
      "Epoch 521/10000 | Loss: 0.0007\n",
      "Epoch 531/10000 | Loss: 0.0007\n",
      "Epoch 541/10000 | Loss: 0.0037\n",
      "Epoch 551/10000 | Loss: 0.0029\n",
      "Epoch 561/10000 | Loss: 0.0006\n",
      "Epoch 571/10000 | Loss: 0.0012\n",
      "Epoch 581/10000 | Loss: 0.0005\n",
      "Epoch 591/10000 | Loss: 0.0013\n",
      "Epoch 601/10000 | Loss: 0.0150\n",
      "Epoch 611/10000 | Loss: 0.0003\n",
      "Epoch 621/10000 | Loss: 0.0016\n",
      "Epoch 631/10000 | Loss: 0.0007\n",
      "Epoch 641/10000 | Loss: 0.0015\n",
      "Epoch 651/10000 | Loss: 0.0013\n",
      "Epoch 661/10000 | Loss: 0.0006\n",
      "Epoch 671/10000 | Loss: 0.0038\n",
      "Epoch 681/10000 | Loss: 0.0001\n",
      "Epoch 691/10000 | Loss: 0.0001\n",
      "Epoch 701/10000 | Loss: 0.0012\n",
      "Epoch 711/10000 | Loss: 0.0009\n",
      "Epoch 721/10000 | Loss: 0.0008\n",
      "Epoch 731/10000 | Loss: 0.0021\n",
      "Epoch 741/10000 | Loss: 0.0003\n",
      "Epoch 751/10000 | Loss: 0.0008\n",
      "Epoch 761/10000 | Loss: 0.0033\n",
      "Epoch 771/10000 | Loss: 0.0021\n",
      "Epoch 781/10000 | Loss: 0.0006\n",
      "Epoch 791/10000 | Loss: 0.0003\n",
      "Epoch 801/10000 | Loss: 0.0038\n",
      "Epoch 811/10000 | Loss: 0.0010\n",
      "Epoch 821/10000 | Loss: 0.0022\n",
      "Epoch 831/10000 | Loss: 0.0166\n",
      "Epoch 841/10000 | Loss: 0.0001\n",
      "Epoch 851/10000 | Loss: 0.0007\n",
      "Epoch 861/10000 | Loss: 0.0007\n",
      "Epoch 871/10000 | Loss: 0.0001\n",
      "Epoch 881/10000 | Loss: 0.0127\n",
      "Epoch 891/10000 | Loss: 0.0006\n",
      "Epoch 901/10000 | Loss: 0.0002\n",
      "Epoch 911/10000 | Loss: 0.0002\n",
      "Epoch 921/10000 | Loss: 0.0011\n",
      "Epoch 931/10000 | Loss: 0.0019\n",
      "Epoch 941/10000 | Loss: 0.0003\n",
      "Epoch 951/10000 | Loss: 0.0003\n",
      "Epoch 961/10000 | Loss: 0.0034\n",
      "Epoch 971/10000 | Loss: 0.0010\n",
      "Epoch 981/10000 | Loss: 0.0000\n",
      "Epoch 991/10000 | Loss: 0.0018\n",
      "Epoch 1001/10000 | Loss: 0.0006\n",
      "Epoch 1011/10000 | Loss: 0.0079\n",
      "Epoch 1021/10000 | Loss: 0.0104\n",
      "Epoch 1031/10000 | Loss: 0.0021\n",
      "Epoch 1041/10000 | Loss: 0.0007\n",
      "Epoch 1051/10000 | Loss: 0.0051\n",
      "Epoch 1061/10000 | Loss: 0.0055\n",
      "Epoch 1071/10000 | Loss: 0.0021\n",
      "Epoch 1081/10000 | Loss: 0.0044\n",
      "Epoch 1091/10000 | Loss: 0.0035\n",
      "Epoch 1101/10000 | Loss: 0.0139\n",
      "Epoch 1111/10000 | Loss: 0.0018\n",
      "Epoch 1121/10000 | Loss: 0.0042\n",
      "Epoch 1131/10000 | Loss: 0.0068\n",
      "Epoch 1141/10000 | Loss: 0.0054\n",
      "Epoch 1151/10000 | Loss: 0.0046\n",
      "Epoch 1161/10000 | Loss: 0.0020\n",
      "Epoch 1171/10000 | Loss: 0.0037\n",
      "Epoch 1181/10000 | Loss: 0.0015\n",
      "Epoch 1191/10000 | Loss: 0.0009\n",
      "Epoch 1201/10000 | Loss: 0.0015\n",
      "Epoch 1211/10000 | Loss: 0.0046\n",
      "Epoch 1221/10000 | Loss: 0.0010\n",
      "Epoch 1231/10000 | Loss: 0.0020\n",
      "Epoch 1241/10000 | Loss: 0.0035\n",
      "Epoch 1251/10000 | Loss: 0.0017\n",
      "Epoch 1261/10000 | Loss: 0.0030\n",
      "Epoch 1271/10000 | Loss: 0.0034\n",
      "Epoch 1281/10000 | Loss: 0.0028\n",
      "Epoch 1291/10000 | Loss: 0.0012\n",
      "Epoch 1301/10000 | Loss: 0.0014\n",
      "Epoch 1311/10000 | Loss: 0.0035\n",
      "Epoch 1321/10000 | Loss: 0.0038\n",
      "Epoch 1331/10000 | Loss: 0.0026\n",
      "Epoch 1341/10000 | Loss: 0.0044\n",
      "Epoch 1351/10000 | Loss: 0.0036\n",
      "Epoch 1361/10000 | Loss: 0.0033\n",
      "Epoch 1371/10000 | Loss: 0.0026\n",
      "Epoch 1381/10000 | Loss: 0.0010\n",
      "Epoch 1391/10000 | Loss: 0.0043\n",
      "Epoch 1401/10000 | Loss: 0.0012\n",
      "Epoch 1411/10000 | Loss: 0.0025\n",
      "Epoch 1421/10000 | Loss: 0.0044\n",
      "Epoch 1431/10000 | Loss: 0.0019\n",
      "Epoch 1441/10000 | Loss: 0.0020\n",
      "Epoch 1451/10000 | Loss: 0.0017\n",
      "Epoch 1461/10000 | Loss: 0.0150\n",
      "Epoch 1471/10000 | Loss: 0.0016\n",
      "Epoch 1481/10000 | Loss: 0.0079\n",
      "Epoch 1491/10000 | Loss: 0.0061\n",
      "Epoch 1501/10000 | Loss: 0.0040\n",
      "Epoch 1511/10000 | Loss: 0.0024\n",
      "Epoch 1521/10000 | Loss: 0.0025\n",
      "Epoch 1531/10000 | Loss: 0.0075\n",
      "Epoch 1541/10000 | Loss: 0.0003\n",
      "Epoch 1551/10000 | Loss: 0.0048\n",
      "Epoch 1561/10000 | Loss: 0.0011\n",
      "Epoch 1571/10000 | Loss: 0.0009\n",
      "Epoch 1581/10000 | Loss: 0.0040\n",
      "Epoch 1591/10000 | Loss: 0.0006\n",
      "Epoch 1601/10000 | Loss: 0.0096\n",
      "Epoch 1611/10000 | Loss: 0.0104\n",
      "Epoch 1621/10000 | Loss: 0.0018\n",
      "Epoch 1631/10000 | Loss: 0.0009\n",
      "Epoch 1641/10000 | Loss: 0.0063\n",
      "Epoch 1651/10000 | Loss: 0.0014\n",
      "Epoch 1661/10000 | Loss: 0.0008\n",
      "Epoch 1671/10000 | Loss: 0.0015\n",
      "Epoch 1681/10000 | Loss: 0.0040\n",
      "Epoch 1691/10000 | Loss: 0.0017\n",
      "Epoch 1701/10000 | Loss: 0.0042\n",
      "Epoch 1711/10000 | Loss: 0.0013\n",
      "Epoch 1721/10000 | Loss: 0.0013\n",
      "Epoch 1731/10000 | Loss: 0.0029\n",
      "Epoch 1741/10000 | Loss: 0.0022\n",
      "Epoch 1751/10000 | Loss: 0.0015\n",
      "Epoch 1761/10000 | Loss: 0.0016\n",
      "Epoch 1771/10000 | Loss: 0.0007\n",
      "Epoch 1781/10000 | Loss: 0.0009\n",
      "Epoch 1791/10000 | Loss: 0.0008\n",
      "Epoch 1801/10000 | Loss: 0.0017\n",
      "Epoch 1811/10000 | Loss: 0.0026\n",
      "Epoch 1821/10000 | Loss: 0.0018\n",
      "Epoch 1831/10000 | Loss: 0.0007\n",
      "Epoch 1841/10000 | Loss: 0.0007\n",
      "Epoch 1851/10000 | Loss: 0.0049\n",
      "Epoch 1861/10000 | Loss: 0.0008\n",
      "Epoch 1871/10000 | Loss: 0.0015\n",
      "Epoch 1881/10000 | Loss: 0.0011\n",
      "Epoch 1891/10000 | Loss: 0.0018\n",
      "Epoch 1901/10000 | Loss: 0.0014\n",
      "Epoch 1911/10000 | Loss: 0.0016\n",
      "Epoch 1921/10000 | Loss: 0.0009\n",
      "Epoch 1931/10000 | Loss: 0.0039\n",
      "Epoch 1941/10000 | Loss: 0.0021\n",
      "Epoch 1951/10000 | Loss: 0.0024\n",
      "Epoch 1961/10000 | Loss: 0.0020\n",
      "Epoch 1971/10000 | Loss: 0.0008\n",
      "Epoch 1981/10000 | Loss: 0.0035\n",
      "Epoch 1991/10000 | Loss: 0.0018\n",
      "Epoch 2001/10000 | Loss: 0.0010\n",
      "Epoch 2011/10000 | Loss: 0.0159\n",
      "Epoch 2021/10000 | Loss: 0.0094\n",
      "Epoch 2031/10000 | Loss: 0.0020\n",
      "Epoch 2041/10000 | Loss: 0.0083\n",
      "Epoch 2051/10000 | Loss: 0.0019\n",
      "Epoch 2061/10000 | Loss: 0.0094\n",
      "Epoch 2071/10000 | Loss: 0.0037\n",
      "Epoch 2081/10000 | Loss: 0.0071\n",
      "Epoch 2091/10000 | Loss: 0.0026\n",
      "Epoch 2101/10000 | Loss: 0.0021\n",
      "Epoch 2111/10000 | Loss: 0.0140\n",
      "Epoch 2121/10000 | Loss: 0.0040\n",
      "Epoch 2131/10000 | Loss: 0.0053\n",
      "Epoch 2141/10000 | Loss: 0.0273\n",
      "Epoch 2151/10000 | Loss: 0.0048\n",
      "Epoch 2161/10000 | Loss: 0.0033\n",
      "Epoch 2171/10000 | Loss: 0.0035\n",
      "Epoch 2181/10000 | Loss: 0.0025\n",
      "Epoch 2191/10000 | Loss: 0.0014\n",
      "Epoch 2201/10000 | Loss: 0.0042\n",
      "Epoch 2211/10000 | Loss: 0.0065\n",
      "Epoch 2221/10000 | Loss: 0.0017\n",
      "Epoch 2231/10000 | Loss: 0.0020\n",
      "Epoch 2241/10000 | Loss: 0.0022\n",
      "Epoch 2251/10000 | Loss: 0.0020\n",
      "Epoch 2261/10000 | Loss: 0.0081\n",
      "Epoch 2271/10000 | Loss: 0.0093\n",
      "Epoch 2281/10000 | Loss: 0.0051\n",
      "Epoch 2291/10000 | Loss: 0.0078\n",
      "Epoch 2301/10000 | Loss: 0.0080\n",
      "Epoch 2311/10000 | Loss: 0.0030\n",
      "Epoch 2321/10000 | Loss: 0.0021\n",
      "Epoch 2331/10000 | Loss: 0.0031\n",
      "Epoch 2341/10000 | Loss: 0.0037\n",
      "Epoch 2351/10000 | Loss: 0.0022\n",
      "Epoch 2361/10000 | Loss: 0.0040\n",
      "Epoch 2371/10000 | Loss: 0.0036\n",
      "Epoch 2381/10000 | Loss: 0.0058\n",
      "Epoch 2391/10000 | Loss: 0.0022\n",
      "Epoch 2401/10000 | Loss: 0.0041\n",
      "Epoch 2411/10000 | Loss: 0.0020\n",
      "Epoch 2421/10000 | Loss: 0.0043\n",
      "Epoch 2431/10000 | Loss: 0.0060\n",
      "Epoch 2441/10000 | Loss: 0.0025\n",
      "Epoch 2451/10000 | Loss: 0.0064\n",
      "Epoch 2461/10000 | Loss: 0.0019\n",
      "Epoch 2471/10000 | Loss: 0.0029\n",
      "Epoch 2481/10000 | Loss: 0.0022\n",
      "Epoch 2491/10000 | Loss: 0.0057\n",
      "Epoch 2501/10000 | Loss: 0.0054\n",
      "Epoch 2511/10000 | Loss: 0.0172\n",
      "Epoch 2521/10000 | Loss: 0.0020\n",
      "Epoch 2531/10000 | Loss: 0.0048\n",
      "Epoch 2541/10000 | Loss: 0.0060\n",
      "Epoch 2551/10000 | Loss: 0.0041\n",
      "Epoch 2561/10000 | Loss: 0.0020\n",
      "Epoch 2571/10000 | Loss: 0.0096\n",
      "Epoch 2581/10000 | Loss: 0.0100\n",
      "Epoch 2591/10000 | Loss: 0.0033\n",
      "Epoch 2601/10000 | Loss: 0.0053\n",
      "Epoch 2611/10000 | Loss: 0.0080\n",
      "Epoch 2621/10000 | Loss: 0.0016\n",
      "Epoch 2631/10000 | Loss: 0.0081\n",
      "Epoch 2641/10000 | Loss: 0.0016\n",
      "Epoch 2651/10000 | Loss: 0.0070\n",
      "Epoch 2661/10000 | Loss: 0.0014\n",
      "Epoch 2671/10000 | Loss: 0.0031\n",
      "Epoch 2681/10000 | Loss: 0.0079\n",
      "Epoch 2691/10000 | Loss: 0.0010\n",
      "Epoch 2701/10000 | Loss: 0.0019\n",
      "Epoch 2711/10000 | Loss: 0.0015\n",
      "Epoch 2721/10000 | Loss: 0.0036\n",
      "Epoch 2731/10000 | Loss: 0.0027\n",
      "Epoch 2741/10000 | Loss: 0.0018\n",
      "Epoch 2751/10000 | Loss: 0.0010\n",
      "Epoch 2761/10000 | Loss: 0.0041\n",
      "Epoch 2771/10000 | Loss: 0.0032\n",
      "Epoch 2781/10000 | Loss: 0.0027\n",
      "Epoch 2791/10000 | Loss: 0.0028\n",
      "Epoch 2801/10000 | Loss: 0.0057\n",
      "Epoch 2811/10000 | Loss: 0.0032\n",
      "Epoch 2821/10000 | Loss: 0.0072\n",
      "Epoch 2831/10000 | Loss: 0.0053\n",
      "Epoch 2841/10000 | Loss: 0.0079\n",
      "Epoch 2851/10000 | Loss: 0.0019\n",
      "Epoch 2861/10000 | Loss: 0.0021\n",
      "Epoch 2871/10000 | Loss: 0.0100\n",
      "Epoch 2881/10000 | Loss: 0.0047\n",
      "Epoch 2891/10000 | Loss: 0.0017\n",
      "Epoch 2901/10000 | Loss: 0.0030\n",
      "Epoch 2911/10000 | Loss: 0.0010\n",
      "Epoch 2921/10000 | Loss: 0.0134\n",
      "Epoch 2931/10000 | Loss: 0.0020\n",
      "Epoch 2941/10000 | Loss: 0.0023\n",
      "Epoch 2951/10000 | Loss: 0.0057\n",
      "Epoch 2961/10000 | Loss: 0.0023\n",
      "Epoch 2971/10000 | Loss: 0.0030\n",
      "Epoch 2981/10000 | Loss: 0.0068\n",
      "Epoch 2991/10000 | Loss: 0.0009\n",
      "Epoch 3001/10000 | Loss: 0.0010\n",
      "Epoch 3011/10000 | Loss: 0.0226\n",
      "Epoch 3021/10000 | Loss: 0.0045\n",
      "Epoch 3031/10000 | Loss: 0.0107\n",
      "Epoch 3041/10000 | Loss: 0.0109\n",
      "Epoch 3051/10000 | Loss: 0.0118\n",
      "Epoch 3061/10000 | Loss: 0.0226\n",
      "Epoch 3071/10000 | Loss: 0.0084\n",
      "Epoch 3081/10000 | Loss: 0.0078\n",
      "Epoch 3091/10000 | Loss: 0.0077\n",
      "Epoch 3101/10000 | Loss: 0.0376\n",
      "Epoch 3111/10000 | Loss: 0.0034\n",
      "Epoch 3121/10000 | Loss: 0.0095\n",
      "Epoch 3131/10000 | Loss: 0.0115\n",
      "Epoch 3141/10000 | Loss: 0.0018\n",
      "Epoch 3151/10000 | Loss: 0.0055\n",
      "Epoch 3161/10000 | Loss: 0.0058\n",
      "Epoch 3171/10000 | Loss: 0.0019\n",
      "Epoch 3181/10000 | Loss: 0.0064\n",
      "Epoch 3191/10000 | Loss: 0.0025\n",
      "Epoch 3201/10000 | Loss: 0.0053\n",
      "Epoch 3211/10000 | Loss: 0.0070\n",
      "Epoch 3221/10000 | Loss: 0.0034\n",
      "Epoch 3231/10000 | Loss: 0.0117\n",
      "Epoch 3241/10000 | Loss: 0.0086\n",
      "Epoch 3251/10000 | Loss: 0.0068\n",
      "Epoch 3261/10000 | Loss: 0.0265\n",
      "Epoch 3271/10000 | Loss: 0.0058\n",
      "Epoch 3281/10000 | Loss: 0.0045\n",
      "Epoch 3291/10000 | Loss: 0.0055\n",
      "Epoch 3301/10000 | Loss: 0.0155\n",
      "Epoch 3311/10000 | Loss: 0.0060\n",
      "Epoch 3321/10000 | Loss: 0.0031\n",
      "Epoch 3331/10000 | Loss: 0.0237\n",
      "Epoch 3341/10000 | Loss: 0.0051\n",
      "Epoch 3351/10000 | Loss: 0.0087\n",
      "Epoch 3361/10000 | Loss: 0.0034\n",
      "Epoch 3371/10000 | Loss: 0.0039\n",
      "Epoch 3381/10000 | Loss: 0.0034\n",
      "Epoch 3391/10000 | Loss: 0.0187\n",
      "Epoch 3401/10000 | Loss: 0.0033\n",
      "Epoch 3411/10000 | Loss: 0.0094\n",
      "Epoch 3421/10000 | Loss: 0.0142\n",
      "Epoch 3431/10000 | Loss: 0.0144\n",
      "Epoch 3441/10000 | Loss: 0.0029\n",
      "Epoch 3451/10000 | Loss: 0.0084\n",
      "Epoch 3461/10000 | Loss: 0.0009\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_and_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_update_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\GitHub\\Machine_Learning\\WebChess\\trainAgentOnFly.py:333\u001b[0m, in \u001b[0;36mtrain_and_save_model\u001b[1;34m(db_path, model_path, epochs, batch_size, gamma, target_update_freq)\u001b[0m\n\u001b[0;32m    331\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    332\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m--> 333\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m target_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    336\u001b[0m     target_model\u001b[38;5;241m.\u001b[39mload_state_dict(model\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "File \u001b[1;32mc:\\Users\\mnsah\\Anaconda3\\envs\\nsahadat\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mnsah\\Anaconda3\\envs\\nsahadat\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\mnsah\\Anaconda3\\envs\\nsahadat\\lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\mnsah\\Anaconda3\\envs\\nsahadat\\lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mnsah\\Anaconda3\\envs\\nsahadat\\lib\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_save_model(epochs=1000, target_update_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvChessNet.load(\"conv_chess_model.pth\")\n",
    "\n",
    "# model = ChessNet.load(\"chess_rl_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a15081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsahadat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
