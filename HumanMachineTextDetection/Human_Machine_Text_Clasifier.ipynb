{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# max_length = model.config.n_positions\n",
    "# stride = 256\n",
    "# seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "# nlls = []\n",
    "# prev_end_loc = 0\n",
    "# for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "#     end_loc = min(begin_loc + max_length, seq_len)\n",
    "#     trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "#     input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "#     target_ids = input_ids.clone()\n",
    "#     target_ids[:, :-trg_len] = -100\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "#         # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "#         # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "#         # to the left by 1.\n",
    "#         neg_log_likelihood = outputs.loss\n",
    "\n",
    "#     nlls.append(neg_log_likelihood)\n",
    "\n",
    "#     prev_end_loc = end_loc\n",
    "#     if end_loc == seq_len:\n",
    "#         break\n",
    "\n",
    "# ppl = torch.exp(torch.stack(nlls).mean())\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "access_token = \"***"\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "# 'wiki_labeled', 'research_abstracts_labeled'\n",
    "dataset1 = load_dataset(\"NicolaiSivesind/human-vs-machine\", \"wiki_labeled\", token = access_token)\n",
    "dataset2 = load_dataset(\"NicolaiSivesind/human-vs-machine\", \"research_abstracts_labeled\", token = access_token)\n",
    "\n",
    "\n",
    "dataset_train = concatenate_datasets([dataset1['train'], dataset2['train']])\n",
    "dataset_validation = concatenate_datasets([dataset1['validation'], dataset2['validation']])\n",
    "\n",
    "print(dataset_train)\n",
    "print(dataset_validation)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_data_train = dataset_train.map(tokenize_function, batched=True)\n",
    "tokenized_data_validation = dataset_validation.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_data_train\n",
    "\n",
    "tokenized_data_validation\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, label = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=label)\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data_train, #.shuffle(seed=42).select(range(1000)),\n",
    "    eval_dataset=tokenized_data_validation, #.shuffle(seed=42).select(range(1000)),\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('./test_trainer/human_machine_classifier')\n",
    "print(trainer.state.best_model_checkpoint)\n",
    "\n",
    "\n",
    "from transformers import pipeline,TextClassificationPipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "pipe = pipeline('text-classification', model='./test_trainer/human_machine_classifier')\n",
    "# prediction = pipe(\"The text to predict\", return_all_scores=True)\n",
    "\n",
    "test = [\"\"\"Collaborative search on the plane without communication presents a novel approach to solving the task of finding a target on a flat surface, without the use of communication among the searchers. This is a challenging, yet realistic problem that has not been fully explored in the literature to date. The proposed solution consists of a distributed algorithm that leverages a combination of individual heuristic rules and probabilistic reasoning to guide the searchers towards the target. Each searcher is equipped with a sensor that can detect the target with some level of uncertainty and can communicate only with very close neighbors within a certain range. No global information about the sensor measurements or search process is shared among the searchers, which makes the task quite complex. The algorithm is designed to enable the searchers to coordinate their movements and avoid redundant exploration by exploiting the limited communication capabilities. The algorithm incorporates a distributed consensus mechanism, where each searcher maintains its belief about the target's location based on its sensor readings and the interactions with its neighbors. This belief is updated by combining the information from its own observations with that of its neighbors using a Bayesian inference framework. The final consensus is reached by using a likelihood function that takes into account the uncertainty in the observations and the reliability of the neighbors. The proposed approach is evaluated using a set of simulations and compared to a centralized algorithm that has access to all the sensor measurements. The results show that the proposed algorithm is able to achieve comparable performance to the centralized algorithm, while using only local information and limited communication. Moreover, the proposed algorithm is shown to be scalable and robust to changes in the search environment, such as the disappearance and sudden reappearance of the target. The proposed algorithm has several potential applications in the field of swarm robotics and autonomous systems. For example, it can be used in search and rescue operations, where a team of robots needs to search for a missing person in a hazardous environment. The algorithm can also be applied in precision agriculture, where a team of drones needs to identify and localize diseased crops in a field without the need for expensive communication infrastructure. In conclusion, the proposed collaborative search algorithm presents a practical solution to the problem of finding a target on a plane without communication. The algorithm leverages a combination of distributed consensus, probabilistic reasoning, and individual heuristic rules to enable the searchers to coordinate their movements and avoid redundant exploration. The algorithm is shown to be robust and scalable, and has potential applications in many real-world scenarios\"\"\"]\n",
    "\n",
    "pipe(test, return_all_scores = True)[0][1]['score']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
